{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment Report: LangChain & NLP Task\n",
    "\n",
    "Objective:\n",
    "The primary goal of this project was to build a question-answering (QA) system using LangChain and an open-source large language model (LLM) such as GPT-2 or GPT-J. The system takes user input and responds with answers based on the provided context.\n",
    "This report outlines the steps taken to:\n",
    "Set up the environment.\n",
    "Integrate an open-source LLM with LangChain to create a functional QA system.\n",
    "Design a custom prompt template to improve the quality of the responses.\n",
    "Implement memory or tool integration to enhance system features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Setting up the Environment\n",
    "Python Environment Setup:\n",
    "To begin, a virtual Python environment was created to manage dependencies in isolation. The following commands were used:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py -m venv langchain_env\n",
    "source langchain_env/bin/activate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Required Libraries:\n",
    "The following Python packages were installed to support the development of the QA system:\n",
    "\n",
    "LangChain: Enables chaining language models together and handling complex language tasks.\n",
    "\n",
    "HuggingFace Transformers: Facilitates access to open-source LLMs like GPT-2 and GPT-J.\n",
    "\n",
    "Torch: A deep learning framework required to run these models.\n",
    "\n",
    "Sentence-Transformers: Used for document embedding, enabling relevant context retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "import transformers\n",
    "import torch\n",
    "import sentence-transformers\n",
    "import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This environment provides the necessary tools to load, run, and integrate the LLM with LangChain to handle QA tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: LLM Integration with LangChain\n",
    "Loading the LLM:\n",
    "For this project, GPT-2 was chosen as the LLM. Using Hugging Face's transformers library, the model and tokenizer were loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load GPT-2 model\n",
    "qa_pipeline\n",
    "\n",
    "qa_pipeline = pipeline('text-generation', model=\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pipeline will serve as the core for generating answers to questions provided by users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrating LangChain for Question Answering:\n",
    "LangChain was used to create a retrieval-based QA system. The system retrieves relevant information from a given context using semantic embeddings and passes the most relevant sections to the LLM for answering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "Vector Store Creation: We used the FAISS vector store and Hugging Face’s Sentence-Transformers for embedding the context into a vector space. This helps retrieve relevant sections from the document based on the user's question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Load embedding model for document retrieval\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Create FAISS vector store\n",
    "doc_store = FAISS(embedding_model.embed_documents, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question Answering Chain: Using LangChain's RetrievalQA, the system retrieves relevant portions of the context and passes it to the LLM. This allows the model to answer questions based on the most pertinent sections of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextGenerationPipeline\n",
    "\n",
    "# Load GPT-2 model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Create a text generation pipeline for LangChain integration\n",
    "hf_pipeline = TextGenerationPipeline(model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Wrap the pipeline with LangChain\n",
    "llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
    "\n",
    "# Setup the QA chain\n",
    "qa_chain = RetrievalQA(llm=llm, retriever=doc_store.as_retriever())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User Interaction: The system accepts user input in the form of a question and retrieves the relevant context to generate an answer. The context must be loaded into the document retriever before answering the user’s query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_qa_system(context, question):\n",
    "    # Add the context to the document retriever\n",
    "    doc_store.add_texts([context])\n",
    "    \n",
    "    # Get the answer\n",
    "    answer = qa_chain.run(input=question)\n",
    "    return answer\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    context = \"Provide a paragraph or short document as context.\"\n",
    "    user_question = input(\"Please enter your question: \")\n",
    "    \n",
    "    result = run_qa_system(context, user_question)\n",
    "   print(f\"Answer: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Working with LangChain Prompt Templates\n",
    "\n",
    "Custom Prompt Template:\n",
    "A custom prompt template was created using LangChain’s PromptTemplate feature. This allows us to control the format and structure of the input that is provided to the LLM, improving the quality and relevance of the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt Template Code:\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "You are an intelligent assistant. Use the following context to answer the question in a concise and accurate manner.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# Define the prompt template\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=template\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the Prompt Template in the QA System:\n",
    "This template structures the input provided to the LLM, ensuring it clearly separates the context, question, and answer sections. By specifying the format, the model generates more accurate and concise answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_qa_with_custom_prompt(context, question):\n",
    "    # Format the prompt using LangChain's PromptTemplate\n",
    "    formatted_prompt = prompt.format(context=context, question=question)\n",
    "    \n",
    "    # Run the QA chain with the formatted prompt\n",
    "    answer = qa_chain.run(input=formatted_prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benefits of the Prompt Template:\n",
    "\n",
    "Improved Structure: Separating the context, question, and answer ensures clarity, making it easier for the LLM to focus on the task.\n",
    "\n",
    "Instruction Control: By specifying the model should be concise and accurate, we guide the model to provide relevant and compact responses.\n",
    "\n",
    "Consistency: Using a standard template ensures that the system maintains consistency in its responses, making the output predictable and easier to refine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Feature Implementation – Memory Integration\n",
    "Memory Integration:\n",
    "\n",
    "To improve the QA system’s interactivity, LangChain’s ConversationBufferMemory was integrated. This feature enables the system to remember past interactions with the user, allowing it to respond to follow-up questions in the context of the entire conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "# Create a memory object to store conversation history\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "\n",
    "# Build the conversational QA chain with memory\n",
    "conversational_chain = ConversationalRetrievalChain(\n",
    "    llm=llm,\n",
    "    retriever=doc_store.as_retriever(),\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "def run_qa_with_memory(context, question):\n",
    "    # Add the context to the document retriever\n",
    "    doc_store.add_texts([context])\n",
    "    \n",
    "    # Run the conversational chain with memory\n",
    "    result = conversational_chain.run(input=question)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Memory Enhances the System:\n",
    "Context Retention: Memory allows the system to track prior interactions, making it capable of answering follow-up questions or clarifications without losing the context.\n",
    "\n",
    "Improved User Experience: Users can have more natural conversations, asking for further explanations or deeper details based on previous answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion:\n",
    "This project successfully implemented a question-answering system using LangChain and an open-source LLM (GPT-2). The system can take a user’s question, retrieve relevant context, and provide a concise answer. Key improvements were achieved through the use of a custom prompt template and memory integration to retain conversation history. The report details:\n",
    "\n",
    "Environment Setup: Documenting the installation of necessary libraries and tools.\n",
    "\n",
    "LLM Integration: Utilizing Hugging Face's GPT-2 and LangChain to create a QA system.\n",
    "\n",
    "Custom Prompt Design: Improving the LLM’s performance by structuring input through prompt templates.\n",
    "\n",
    "Memory Integration: Enhancing the system’s interactivity by allowing follow-up questions and ongoing dialogue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Enhancements:\n",
    "In the future, the system could be further enhanced by integrating external tools, such as search engines or APIs, to handle more complex queries when the provided context is insufficient. Additionally, fine-tuning the LLM for specific domains could improve answer accuracy and relevance in specialized fields.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
